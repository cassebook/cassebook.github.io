@inproceedings{Adavanne:2016,
        title={Sound event detection in multichannel audio using spatial and harmonic features},
        author={Adavanne, S. and Parascandolo, G. and Pertila, P. and Heittola, T. and Virtanen, T.},
        booktitle={Proc DCASE 2016},
        url={http://www.cs.tut.fi/sgn/arg/dcase2016/documents/challenge\_technical\_reports/Task3/Adavanne\_2016\_task3.pdf},
        abstract={In this paper, we propose the use of spatial and harmonic features in combination with long short term memory (LSTM) recurrent neural network (RNN) for automatic sound event detection (SED) task. Real life sound recordings typically have many overlapping sound events, making it hard to recognize with just mono channel audio. Human listeners have been successfully recognizing the mixture of overlapping sound events using pitch cues and exploiting the stereo (multichannel) audio signal available at their ears to spatially localize these events. Traditionally SED systems have only been using mono channel audio, motivated by the human listener we propose to extend them to use multichannel audio. The proposed SED system is compared against the state of the art mono channel method on the development subset of TUT sound events detection 2016 database [1]. The proposed method improves the F-score by 3.75% while reducing the error rate by 6%.},
}

@article{Anguera:2012,
  title={Speaker diarization: A review of recent research},
  author={Anguera, Xavier and Bozonnet, Simon and Evans, Nicholas and Fredouille, Corinne and Friedland, Gerald and Vinyals, Oriol},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={20},
  number={2},
  pages={356--370},
  year={2012},
  publisher={IEEE},
  abstract={Speaker diarization is the task of determining “who spoke when?” in an audio or video recording that contains an unknown amount of speech and also an unknown number of speakers. Initially, it was proposed as a research topic related to automatic speech recognition, where speaker diarization serves as an upstream processing step. Over recent years, however, speaker diarization has become an important key technology for many tasks, such as navigation, retrieval, or higher level inference on audio data. Accordingly, many important improvements in accuracy and robustness have been reported in journals and conferences in the area. The application domains, from broadcast news, to lectures and meetings, vary greatly and pose different problems, such as having access to multiple microphones and multimodal information or overlapping speech. The most recent review of existing technology dates back to 2006 and focuses on the broadcast news domain. In this paper, we review the current state-of-the-art, focusing on research developed since 2006 that relates predominantly to speaker diarization for conference meetings. Finally, we present an analysis of speaker diarization performance as reported through the NIST Rich Transcription evaluations on meeting data and identify important areas for future research.},
}

@article{Aucouturier:2007,
        Author = {Aucouturier, J.-J. and Pachet, F.},
        Journal = {J. Acoustical Soc. of America},
        Keywords = {Soundscapes, Music, Similarity, Classification, Homogeneity, Distribution},
        volume = {122},
        pages = {881--891},
        number = {2},
        Title = {The bag-of-frames approach to audio pattern recognition: a sufficient model for urban soundscapes but not for polyphonic music},
        Year = {2006}
}

@article{Barber:2010,
        Author = {Barber, D. and Cemgil, A. T.},
        Journal = {{IEEE} Signal Processing Magazine},
        Number = {6},
        Pages = {18--28},
        Rating = {5},
        Read = {Yes},
        Title = {Graphical models for time-series},
        Volume = {27},
        Year = {2010},
        Abstract = {Time-series analysis is central to many problems in signal processing, including acoustics, image processing, vision, tracking, information retrieval, and finance, to name a few. Because of the wide base of application areas, having a common description of the models is useful in transferring ideas between the various communities. Graphical models provide a compact way to represent such models and thereby rapidly transfer ideas. We will discuss briefly how classical timeseries models such as Kalman filters and hidden Markov models (HMMs) can be represented as graphical models and critically how this representation differs from other common graphical representations such as state-transition and block diagrams. We will use this framework to show how one may easily envisage novel models and gain insight into their computational implementation.
},
}

@inproceedings{Battaglino:2015,
  title={ACOUSTIC CONTEXT RECOGNITION FOR MOBILE DEVICES USING A REDUCED COMPLEXITY {SVM}},
  author={Battaglino, Daniele and Mesaros, Annamaria and Lepauloux, Ludovick and Pilati, Laurent and Evans, Nicholas},
  booktitle={Proc EUSIPCO 2015},
  pages={534--538},
  year={2015},
  abstract={Automatic context recognition enables mobile devices to re-
act to changes in the environment and different situations.
While many different sensors can be used for context recogni-
tion, the use of acoustic cues is among the most popular and
successful. Current approaches to acoustic context recogni-
tion (ACR) are too costly in terms of computation and mem-
ory requirements to support an always-listening mode. This
paper describes our work to develop a reduced complexity,
efficient approach to ACR involving support vector machine
classifiers. The principal hypothesis is that a significant frac-
tion of training data contains information redundant to clas-
sification. Through clustering, training data can thus be se-
lectively decimated in order to reduce the number of support
vectors needed to represent discriminative hyperplanes. This
represents a significant saving in terms of computational and
memory efficiency, with only modest degradations in classifi-
cation accuracy.},
}

@inproceedings{Battaglino:2016,
  title={The open-set problem in acoustic scene classification},
  author={Battaglino, Daniele and Lepauloux, Ludovick and Evans, Nicholas},
  booktitle={IEEE Int. Workshop on Acoustic Signal Enhancement (IWAENC)},
  year={2016}
}

@article{Bello:2005,
        Author = {Bello, J. P. and Daudet, L. and Abdallah, S. and Duxbury, C. and Davies, M. and Sandler, M. B.},
        Isbn = {1063-6676},
        Journal = {IEEE Transactions on Speech and Audio Processing},
        Number = {5},
        Pages = {1035--1047},
        Title = {A Tutorial on Onset Detection in Music Signals},
        Volume = {13},
        Year = {2005},
        Abstract = {Note onset detection and localization is useful in a number of analysis and indexing techniques for musical signals. The usual way to detect onsets is to look for \'transient\' regions in the signal, a notion that leads to many definitions: a sudden burst of energy, a change in the short-time spectrum of the signal or in the statistical properties, etc. The goal of this paper is to review, categorize, and compare some of the most commonly used techniques for onset detection, and to present possible enhancements. We discuss methods based on the use of explicitly predefined signal features: the signal's amplitude envelope, spectral magnitudes and phases, time-frequency representations; and methods based on probabilistic signal models: model-based change point detection, surprise signals, etc. Using a choice of test cases, we provide some guidelines for choosing the appropriate method for a given application.},
}

@inproceedings{Benetos:2012,
  address = {York, UK},
  author = {E. Benetos and M. Lagrange and S. Dixon},
  booktitle = {15th Int. Conf. on Digital Audio Effects (DAFx)},
  title = {Characterisation of acoustic scenes using a temporally-constrained shift-invariant model},
  Pages = {317--323},
  month = {September},
  year = {2012}
}


@inproceedings{Benetos:2016,
  title={Detection of Overlapping Acoustic Events using a Temporally-Constrained Probabilistic Model},
  author={Benetos, E. and Lafay, G. and Lagrange, M. and Plumbley, M.},
  booktitle={IEEE Int. Conf. on Acoustics, Speech, and Signal Processing},
  year={2016},
  pages={6450--6454},
  abstract={In this paper, a system for overlapping acoustic event detection is proposed, which models the temporal evolution of sound events. The system is based on probabilistic latent component analysis, supporting the use of a sound event dictionary where each exemplar consists of a succession of spectral templates. The temporal succession of the templates is controlled through event class-wise Hidden Markov Models (HMMs). As input time/frequency representation, the Equivalent Rectangular Bandwidth (ERB) spectrogram is used. Experiments are carried out on polyphonic datasets of office sounds generated using an acoustic scene synthesizer-simulator, as well as real and synthesized monophonic datasets for comparative purposes. Results show that the proposed system outperforms several state-of-the-art methods for overlapping acoustic event detection on the same task, using both frame-based and event-based metrics, and is robust to varying event density and noise levels. },
}

@article{Benetos:2016b,
	author = {E. Benetos and G. Lafay and M. Lagrange and M. D. Plumbley},
    title = {Polyphonic sound event tracking using linear dynamical systems},
    journal = {IEEE/ACM Transactions on Audio, Speech and Language Processing},
    year={accepted}
}

@ARTICLE{Bengio:2003,
   author = {Y. Bengio and R. Ducharme and P. Vincent and C. Jauvin},
    title = "{A Neural Probabilistic Language Model}",
  journal = {J. Machine Learning Research},
  volume    = {3},
  year      = {2003},
  pages = {1137--1155},
  month = feb
}


@article{Beritelli:2002,
        Author = {Beritelli, F. and Casale, S. and Ruggeri, G. and Serrano, S.},
        Date-Added = {2008-11-03 09:49:48 +0000},
        Date-Modified = {2008-11-03 10:44:48 +0000},
        Journal = {IEEE Signal Processing Letters},
        Number = {3},
        Pages = {85--88},
        Read = {Yes},
        Title = {Performance evaluation and comparison of {G.729}/{AMR}/fuzzy voice activity detectors},
        Volume = {9},
        Year = {2002},
        abstract={The paper proposes a performance evaluation and comparison of G.729, AMR, and fuzzy voice activity detection (FVAD) algorithms. The comparison was made using objective, psychoacoustic, and subjective parameters. A highly varied speech database was also set up to evaluate the extent to which VADs depend on language, the signal-to-noise ratio (SNR), or the power level.},
}

@article{Bischof:2010,
  title={Autonomous Audio-Supported Learning of Visual Classifiers for Traffic Monitoring.},
  author={Bischof, Horst and Godec, Martin and Leistner, Christian and Rinner, Bernhard and Starzacher, Andreas},
  journal={IEEE Intelligent Systems},
  volume={25},
  number={3},
  pages={15--23},
  year={2010}
}

@INPROCEEDINGS{Bisot:2015,
author={V. Bisot and S. Essid and G. Richard},
booktitle={23rd European Signal Processing Conf. (EUSIPCO)},
title={HOG and subband power distribution image features for acoustic scene classification},
year={2015},
pages={719-723},
keywords={acoustic imaging;acoustic signal processing;audio signal processing;gradient methods;image classification;support vector machines;HOG;SPD;Sinkhorn kernel;acoustic scene classification;acoustic scene dataset;audio scenes;earth mover distance kernel;histogram of gradients;spectrogram image;subband power distribution image features;support vector machines;Acoustics;Feature extraction;Histograms;Kernel;Spectrogram;Support vector machines;Time-frequency analysis;Acoustic scene classification;Sinkhorn distance;subband power distribution image;support vector machine},
month={Aug},}

@INPROCEEDINGS{Bisot:2016,
author={V. Bisot and R. Serizel and S. Essid and G. Richard},
booktitle={2016 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)},
title={Acoustic scene classification with matrix factorization for unsupervised feature learning},
year={2016},
pages={6445-6449},
keywords={acoustic imaging;image classification;matrix decomposition;time-frequency analysis;unsupervised learning;acoustic environment recording;acoustic scene classification;image decomposition;large ASC dataset;matrix factorization;projection coefficients;time-frequency image;unsupervised feature learning;Acoustics;Dictionaries;Matrix decomposition;Principal component analysis;Sparse matrices;Spectrogram;Time-frequency analysis;Acoustic scene classification;matrix factorization;unsupervised feature learning},
month={March},}

@inproceedings{Boulanger-Lewandowski:2012,
	Author = {N. Boulanger-Lewandowski and Y. Bengio and P. Vincent},
	Booktitle = {29th Int. Conf. on Machine Learning},
	Address = {Edinburgh, Scotland, UK},
	Title = {Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription},
	Year = {2012}}

@article{Cai:2006,
  title={A flexible framework for key audio effects detection and auditory context inference},
  author={Cai, Rui and Lu, Lie and Hanjalic, Alan and Zhang, Hong-Jiang and Cai, Lian-Hong},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={14},
  number={3},
  pages={1026--1039},
  year={2006},
  publisher={IEEE}
}

@inproceedings{Cakir:2015,
  title={Polyphonic sound event detection using multi label deep neural networks},
  author={Cakir, Emre and Heittola, Toni and Huttunen, Heikki and Virtanen, Tuomas},
  booktitle={Int. Joint Conf. on Neural Networks (IJCNN)},
  pages={1--7},
  year={2015},
  doi={10.1109/IJCNN.2015.7280624},
  abstract={In this paper, the use of multi label neural networks are proposed for detection of temporally overlapping sound events in realistic environments. Real-life sound recordings typically have many overlapping sound events, making it hard to recognize each event with the standard sound event detection methods. Frame-wise spectral-domain features are used as inputs to train a deep neural network for multi label classification in this work. The model is evaluated with recordings from realistic everyday environments and the obtained overall accuracy is 63.8%. The method is compared against a state-of-the-art method using non-negative matrix factorization as a pre-processing stage and hidden Markov models as a classifier. The proposed method improves the accuracy by 19% percentage points overall.},
}

@inproceedings{Cakir:2015b,
  title={Multi-label vs. combined single-label sound event detection with deep neural networks},
  author={Cakir, Emre and Heittola, Toni and Huttunen, Heikki and Virtanen, Tuomas},
  booktitle={23rd European Signal Processing Conf. (EUSIPCO)},
  pages={2551--2555},
  year={2015},
  abstract={In real-life audio scenes, many sound events from different sources are simultaneously active, which makes the automatic sound event detection challenging. In this paper, we compare two different deep learning methods for the detection of environmental sound events: combined single-label classification and multi-label classification. We investigate the accuracy of both methods on the audio with different levels of polyphony. Multi-label classification achieves an overall 62.8% accuracy, whereas combined single-label classification achieves a very close 61.9% accuracy. The latter approach offers more flexibility on real-world applications by gathering the relevant group of sound events in a single classifier with various combinations.}
}

@INPROCEEDINGS{Cauchi:2013,
author={B. Cauchi and M. Lagrange and N. Misdariis and A. Cont},
booktitle={2013 14th Int. Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS)},
title={Saliency-based modeling of acoustic scenes using sparse non-negative matrix factorization},
year={2013},
doi={10.1109/WIAMIS.2013.6616131},
ISSN={2158-5873},
month={July}
}

@inproceedings{Cotton:2011,
  title={Spectral vs. Spectro-Temporal Features for Acoustic Event Classification},
  author={C. V. Cotton and D. P. W. Ellis}, 
  booktitle={IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)}, 
  pages={69--72}, 
  month = oct,
  year={2011},
}

@book{DCASE:2016,
    title = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)",
    author = "Tuomas Virtanen and Annamaria Mesaros and Toni Heittola and Plumbley, {Mark D.} and Peter Foster and Emmanouil Benetos and Mathieu Lagrange",
    year = "2016",
    url = {http://www.cs.tut.fi/sgn/arg/dcase2016/},
    publisher = "Tampere University of Technology. Department of Signal Processing",
}

@article{Dennis:2013,
  title={Overlapping sound event recognition using local spectrogram features and the generalised {H}ough transform},
  author={Dennis, Jonathan and Tran, Huy Dat and Chng, Eng Siong},
  journal={Pattern Recognition Letters},
  volume={34},
  number={9},
  pages={1085--1093},
  year={2013},
  publisher={Elsevier},
  doi={10.1016/j.patrec.2013.02.015},
  abstract={In this paper, we address the challenging task of simultaneous recognition of overlapping sound events from single channel audio. Conventional frame-based methods are not well suited to the problem, as each time frame contains a mixture of information from multiple sources. Missing feature masks are able to improve the recognition in such cases, but are limited by the accuracy of the mask, which is a non-trivial problem. In this paper, we propose an approach based on Local Spectrogram Features (LSFs) which represent local spectral information that is extracted from the two-dimensional region surrounding “keypoints” detected in the spectrogram. The keypoints are designed to locate the sparse, discriminative peaks in the spectrogram, such that we can model sound events through a set of representative LSF clusters and their occurrences in the spectrogram. To recognise overlapping sound events, we use a Generalised Hough Transform (GHT) voting system, which sums the information over many independent keypoints to produce onset hypotheses, that can detect any arbitrary combination of sound events in the spectrogram. Each hypothesis is then scored against the class distribution models to recognise the existence of the sound in the spectrogram. Experiments on a set of five overlapping sound events, in the presence of non-stationary background noise, demonstrate the potential of our approach.},
}

@INPROCEEDINGS{Dessein:2010,
  author =	 {A. Dessein and A. Cont and G. Lemaitre},
  title =	 {Real-time polyphonic music transcription with non-negative matrix factorization and beta-divergence},
  booktitle =	 {Int. Soc. for Music Information Retrieval Conf.},
  month =	 aug,
  pages =	 {489--494},
  year =	 {2010}
}

@article{Dewar:2012,
  title={Inference in hidden {Markov} models with explicit state duration distributions},
  author={Dewar, Michael and Wiggins, Chris and Wood, Frank},
  journal={IEEE Signal Processing Letters},
  volume={19},
  number={4},
  pages={235--238},
  year={2012},
  abstract={In this letter, we borrow from the inference techniques developed for unbounded state-cardinality (nonparametric) variants of the HMM and use them to develop a tuning-parameter free, black-box inference procedure for explicit-state-duration hidden Markov models (EDHMM). EDHMMs are HMMs that have latent states consisting of both discrete state-indicator and discrete state-duration random variables. In contrast to the implicit geometric state duration distribution possessed by the standard HMM, EDHMMs allow the direct parameterization and estimation of per-state duration distributions. As most duration distributions are defined over the positive integers, truncation or other approximations are usually required to perform EDHMM inference.},
}

@article{Dietterich:1997,
  title={Solving the multiple instance problem with axis-parallel rectangles},
  author={Dietterich, Thomas G and Lathrop, Richard H and Lozano-P{\'e}rez, Tom{\'a}s},
  journal={Artificial Intelligence},
  volume={89},
  number={1},
  pages={31--71},
  year={1997},
  publisher={Elsevier}
}

@inproceedings{Diment:2013,
        title={SOUND EVENT DETECTION FOR OFFICE LIVE AND OFFICE SYNTHETIC {AASP} CHALLENGE},
        author={Diment, A. and Heittola, T. and Virtanen, T.},
        year={2013},
        howpublished={Extended abstract},
        booktitle={IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (WASPAA 2013 special session)},
  abstract={We present a sound event detection system based on hidden Markov models. The system is evaluated with development material provided in the AASP Challenge on Detection and Classification of Acoustic Scenes and Events. Two approaches using the same basic detection scheme are presented. First one, developed for acoustic scenes with non-overlapping sound events is evaluated with Office Live development dataset. Second one, developed for acoustic scenes with some degree of overlapping sound events is evaluated with Office Synthetic development dataset.},
}


@inproceedings{Diment:2015,
author={A. Diment and E. Cakir and T. Heittola and T. Virtanen},
booktitle={2015 23rd European Signal Processing Conf. (EUSIPCO)},
title={Automatic recognition of environmental sound events using all-pole group delay features},
year={2015},
pages={729--733},
keywords={music;neural nets;speech processing;speech recognition;DNN;all-pole group delay features;automatic recognition;diverse real-life dataset;environmental audio;environmental sound event recognition;group delay function;multilabel deep neural network;music analysis;phase information;speech analysis;Computational modeling;Delays;Discrete cosine transforms;Europe;Feature extraction;Neural networks;Signal processing;Phase spectrum;audio classification;neural networks;sound event recognition},
month={Aug},}

@article{Eronen:2006,
  title={Audio-based context recognition},
  author={Eronen, A. J. and Peltonen, V. T. and Tuomi, J. T. and Klapuri, A. P. and Fagerlund, S. and Sorsa, T. and Lorho, G. and Huopaniemi, J.},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={14},
  number={1},
  pages={321--329},
  year={2006},
}


@INPROCEEDINGS{Foster:2015,
author={P. Foster and S. Sigtia and S. Krstulovic and J. Barker and M. D. Plumbley},
booktitle={IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
title={Chime-home: A dataset for sound source recognition in a domestic environment},
year={2015},
month={Oct}
}

@INPROCEEDINGS{Geiger:2013,
author={J. T. Geiger and B. Schuller and G. Rigoll},
booktitle={2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
title={Large-scale audio feature extraction and {SVM} for acoustic scene classification},
year={2013},
pages={1-4},
keywords={audio signals;feature extraction;signal classification;statistical analysis;support vector machines;D-CASE;IEEE AASP;Mel spectra;SVM;acoustic scene classification;acoustic scenes;large-scale audio feature extraction;latent perceptual indexing;nearest neighbour classifier;scene classification track;short segments;sliding window approach;support vector machines;t-statistic;variable recordings;Accuracy;Feature extraction;Mel frequency cepstral coefficient;Support vector machines;Training;Training data;Computational auditory scene analysis;acoustic scene recognition;feature extraction},
ISSN={1931-1168},
month={Oct},}

@INPROCEEDINGS{Gemmeke:2013,
author={Gemmeke, J. F. and Vuegen, L. and Karsmakers, P. and Vanrumste, B. and Van hamme, H.},
booktitle={IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
title={An exemplar-based {NMF} approach to audio event detection},
year={2013},
month=oct
}

@article{Gill:2016,
        doi = {10.1111/2041-210x.12610},
        year = 2016,
        month = {jun},
        publisher = {Wiley-Blackwell},
        author = {Lisa F. Gill and Pietro B. D'Amelio and Nicolas M. Adreani and Hannes Sagunsky and Manfred C. Gahr and Andries ter Maat},
        title = {A minimum-impact, flexible tool to study vocal communication of small animals with precise individual-level resolution},
        journal = {Methods in Ecology and Evolution},
  abstract={    To understand both proximate and ultimate factors shaping vocal communication, it is fundamental to obtain reliable information of participating individuals on different levels: First, it is necessary to separate and assign the individuals' vocalisations. Secondly, the precise timing of vocal events needs to be retained. Thirdly, vocal behaviour should be recorded from undisturbed animals in meaningful settings. A growing number of studies used animal-attached microphones to tackle these issues, but the implications for the study species and the research question often receded into the background. Here, we aim to initiate a discussion about the limitations, possible applications and the broader potential of such methods.
    Using lightweight wireless microphone backpacks (0·75 g including customised leg-loop harness) combined with multi-channel recording equipment, we captured vocal behaviour of small songbirds. We evaluated the effect of the devices at various levels, including an assessment of how vocal and locomotor activities were affected by initial device attachment and battery exchange. We compared our approach to existing studies and identified suitable research examples.
    We acquired continuous vocalisation recordings of zebra finches, and unequivocally assigned them to interacting individuals, with system-based synchrony, irrespective of background noise. We found effects of initial backpack attachment and of battery replacement on vocal and locomotor activity, but they were minimised through the extended recording duration (ca. 16 days) that outlasted habituation effects (ca. 3 days).
    This method provides the tools to integrate individual vocal communications into a group setting, while enabling animals to behave freely in undisturbed, structured and acoustically complex environments. By minimising the effects on the animals, the behaviour under study, and ultimately on the research question, this approach will revolutionise the ability to capture individual-level vocalisations in a variety of communication contexts, opening up many new opportunities to address novel research questions.},
}

@inproceedings{Heittola:2010,
  title={Audio context recognition using audio event histograms},
  author={Heittola, Toni and Mesaros, Annamaria and Eronen, Antti and Virtanen, Tuomas},
  booktitle={18th European Signal Processing Conf.},
  pages={1272--1276},
  year={2010},
  abstract={This paper presents a method for audio context recognition, meaning classification between everyday environments. The method is based on representing each audio context using a histogram of audio events which are detected using a supervised classifier. In the training stage, each context is modeled with a histogram estimated from annotated training data. In the testing stage, individual sound events are detected in the unknown recording and a histogram of the sound event occurrences is built. Context recognition is performed by computing the cosine distance between this histogram and event histograms of each context from the training database. Term frequency-inverse document frequency weighting is studied for controlling the importance of different events in the histogram distance calculation. An average classification accuracy of 89% is obtained in the recognition between ten everyday contexts. Combining the event based context recognition system with more conventional audio based recognition increases the recognition rate to 92%.},
}


@inproceedings{Heittola:2011,
  title={Sound event detection in multisource environments using source separation},
  author={Heittola, T. and Mesaros, A. and Virtanen, T. and Eronen, A.},
  booktitle={Workshop on Machine Listening in Multisource Environments (CHiME 2011)},
  pages={36--40},
  year={2011},
}

@article{Heittola:2013,
  title={Context-dependent sound event detection},
  author={Heittola, T. and Mesaros, A. and Eronen, A. and Virtanen, T.},
  journal={EURASIP J. Audio, Speech, and Music Processing},
  volume={2013},
  number={1},
  pages={1},
  year={2013},
  publisher={Springer},
  doi={10.1186/1687-4722-2013-1},
  abstract={The work presented in this article studies how the context information can be used in the automatic sound event detection process, and how the detection system can benefit from such information. Humans are using context information to make more accurate predictions about the sound events and ruling out unlikely events given the context. We propose a similar utilization of context information in the automatic sound event detection process. The proposed approach is composed of two stages: automatic context recognition stage and sound event detection stage. Contexts are modeled using Gaussian mixture models and sound events are modeled using three-state left-to-right hidden Markov models. In the first stage, audio context of the tested signal is recognized. Based on the recognized context, a context-specific set of sound event classes is selected for the sound event detection stage. The event detection stage also uses context-dependent acoustic models and count-based event priors. Two alternative event detection approaches are studied. In the first one, a monophonic event sequence is outputted by detecting the most prominent sound event at each time instance using Viterbi decoding. The second approach introduces a new method for producing polyphonic event sequence by detecting multiple overlapping sound events using multiple restricted Viterbi passes. A new metric is introduced to evaluate the sound event detection performance with various level of polyphony. This combines the detection accuracy and coarse time-resolution error into one metric, making the comparison of the performance of detection algorithms simpler. The two-step approach was found to improve the results substantially compared to the context-independent baseline system. In the block-level, the detection accuracy can be almost doubled by using the proposed context-dependent event detection. },
}

@inproceedings{Imoto:2013,
  title={Acoustic scene analysis based on latent acoustic topic and event allocation},
  author={Imoto, Keisuke and Ohishi, Yasunori and Uematsu, Hisashi and Ohmuro, Hitoshi},
  booktitle={2013 IEEE Int. Workshop on Machine Learning for Signal Processing (MLSP)},
  pages={1--6},
  year={2013},
  organization={IEEE},
  doi={10.1109/MLSP.2013.6661957},
  abstract={We propose a model for analyzing acoustic scenes by using long-term (more than several seconds) acoustic signals based on a probabilistic generative model of an acoustic feature sequence associated with acoustic scenes (e.g. “cooking”) and acoustic events (e.g. “cutting with a knife,” “heating a skillet” or “running water”) called latent acoustic topic and event allocation (LATEA) model. The proposed model allows the analysis of a wide variety of sounds and the capture of abstract acoustic scenes by representing acoustic events and scenes as latent variables, and can also describe the acoustic similarity and variance between acoustic events by representing acoustic features as a mixture of Gaussian components. Experiments with real-life sounds indicated that the proposed model exhibited lower perplexity than conventional models; it improved the stability of acoustic scene estimation. The experimental results also suggested that the proposed model can better describe the acoustic similarity and variance between acoustic events than conventional models.},
}

@inproceedings{Imoto:2015,
  title={Acoustic scene analysis from acoustic event sequence with intermittent missing event},
  author={Imoto, Keisuke and Ono, Nobutaka},
  booktitle={2015 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={156--160},
  year={2015},
  organization={IEEE}
}

@article{Johnson:2013,
  title={Bayesian nonparametric hidden semi-{M}arkov models},
  author={Johnson, Matthew J and Willsky, Alan S},
  journal={J. Machine Learning Research},
  volume={14},
  number={Feb},
  pages={673--701},
  year={2013},
  abstract={There is much interest in the Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) as a natural Bayesian nonparametric extension of the ubiquitous Hidden Markov Model for learning from sequential and time-series data. However, in many settings the HDP-HMM's strict Markovian constraints are undesirable, particularly if we wish to learn or encode non-geometric state durations. We can extend the HDP-HMM to capture such structure by drawing upon explicit-duration semi-Markov modeling, which has been developed mainly in the parametric non-Bayesian setting, to allow construction of highly interpretable models that admit natural prior information on state durations.

In this paper we introduce the explicit-duration Hierarchical Dirichlet Process Hidden semi-Markov Model (HDP-HSMM) and develop sampling algorithms for efficient posterior inference. The methods we introduce also provide new methods for sampling inference in the finite Bayesian HSMM. Our modular Gibbs sampling methods can be embedded in samplers for larger hierarchical Bayesian models, adding semi-Markov chain modeling as another tool in the Bayesian inference toolbox. We demonstrate the utility of the HDP-HSMM and our inference methods on both synthetic and real experiments. },
}

@inproceedings{Kim:2009,
  title={Acoustic topic model for audio information retrieval},
  author={Kim, Samuel and Narayanan, Shrikanth and Sundaram, Shiva},
  booktitle={2009 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
  pages={37--40},
  year={2009},
  organization={IEEE}
}

@article{Kumar:2016,
  title={Audio Event Detection using Weakly Labeled Data},
  author={Kumar, Anurag and Raj, Bhiksha},
  journal={arXiv preprint arXiv:1605.02401},
  year={2016}
}

@article{Lagrange:2015,
   author = "Lagrange, Mathieu and Lafay, Grégoire and D\'{e}fr\'{e}ville, Boris and Aucouturier, Jean-Julien",
   title = "The bag-of-frames approach: A not so sufficient model for urban soundscapes",
   journal = "J. Acoustical Soc. of America",
   year = "2015",
   volume = "138",
   number = "5", 
   pages = "EL487-EL492"
}


@article{Lee:1999,
  title={Learning the parts of objects by non-negative matrix factorization},
  author={Lee, Daniel D and Seung, H Sebastian},
  journal={Nature},
  volume={401},
  number={6755},
  pages={788--791},
  year={1999},
  publisher={Nature Publishing Group},
  doi={10.1038/44565},
  abstract={Is perception of the whole based on perception of its parts? There is psychological1 and physiological2, 3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4, 5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
}

@INPROCEEDINGS{Lee:2013,
author={K. Lee and Z. Hyung and J. Nam},
booktitle={2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
title={Acoustic scene classification using sparse feature learning and event-based pooling},
year={2013},
pages={1-4},
keywords={Boltzmann machines;acoustic signal processing;cepstral analysis;learning (artificial intelligence);signal classification;IEEE AASP Challenge development set;MFCC;acoustic scene classification;event-based pooling;machine recognition tasks;mel-frequency cepstral coefficients;sparse feature learning algorithm;sparse restricted Boltzmann machine;unsupervised learning algorithm;Accuracy;Acoustics;Conf.s;Electron tubes;Feature extraction;Mathematical model;Training;acoustic scene classification;environmental sound;event detection;feature learning;max-pooling;restricted Boltzmann machine;sparse feature representation},
ISSN={1931-1168},
month={Oct},}


@inproceedings{Logan:1998,
  title={Factorial HMMs for acoustic modeling},
  author={Logan, Beth and Moreno, Pedro},
  booktitle={IEEE Int. Conf. on Acoustics, Speech and Signal Processing},
  volume={2},
  pages={813--816},
  year={1998},
  organization={IEEE},
  abstract={In the machine learning research field several extensions of hidden Markov models (HMMs) have been proposed. In this paper we study their possibilities and potential benefits for the field of acoustic modeling. We describe preliminary experiments using an alternative modeling approach known as factorial hidden Markov models (FHMMs). We present these models as extensions of HMMs and detail a modification to the original formulation which seems to allow a more natural fit to speech. We present experimental results on the phonetically balanced TIMIT database comparing the performance of FHMMs with HMMs. We also study alternative feature representations that might be more suited to FHMMs.},
}

@article{Lu:2015,
  title={Context-based environmental audio event recognition for scene understanding},
  author={Lu, Tong and Wang, Gongyou and Su, Feng},
  journal={Multimedia Systems},
  volume={21},
  number={5},
  pages={507--524},
  year={2015},
  publisher={Springer},
  doi={10.1007/s00530-014-0424-7},
  abstract={Automatic audio content recognition has attracted an increasing attention for developing multimedia systems, for which the most popular approaches combine frame-based features with statistic models or discriminative classifiers. The existing methods are effective for clean single-source event detection but may not perform well for unstructured environmental sounds, which have a broad noise-like flat spectrum and a diverse variety of compositions. We present an automatic acoustic scene understanding framework that detects audio events through two hierarchies, acoustic scene recognition and audio event recognition, in which the former is preceded by following dominant audio sources and in turn helps infer non-dominant audio events within the same scene through modeling their occurrence correlations. On the scene recognition hierarchy, we perform adaptive segmentation and feature extraction for every input acoustic scene stream through Eigen-audiospace and an optimized feature subspace, respectively. After filtering background, scene streams are recognized by modeling the observation density of dominant features using a two-level hidden Markov model. On the audio event recognition hierarchy, scene knowledge is characterized by an audio context model that essentially describes the occurrence correlations of dominant and non-dominant audio events within this scene. Monte Carlo integration and gradient descent techniques are employed to maximize the likelihood and correctly tag each audio event. To the best of our knowledge, this is the first work that models event correlations as scene context for robust audio event detection from complex and noisy environments. Note that according to the recent report, the mean accuracy for the acoustic scene classification task by human listeners is only around 71 % on the data collected in office environments from the DCASE dataset. None of the existing methods performs well on all scene categories and the average accuracy of the best performances of the recent 11 methods is 53.8 %. The proposed method averagely achieves an accuracy of 62.3 % on the same dataset. Additionally, we create a 10-CASE dataset by manually collecting 5,250 audio clips of 10 scene types and 21 event categories. Our experimental results on 10-CASE show that the proposed method averagely achieves the enhanced performance of 78.3 %, and the average accuracy of audio event recognition can be effectively improved by capturing dominant audio sources and reasoning non-dominant events from the dominant ones through acoustic context modeling. In the future work, exploring the interactions between acoustic scene recognition and audio event detection, and incorporating other modalities to improve the accuracy are required to further advance the proposed framework.},
}

@book{Marler:2004,
  title={Nature's Music: the Science of Birdsong},
  author={Marler, P. R. and Slabbekoorn, H.},
  year={2004},
  publisher={Academic Press},
  address={Massachusetts, USA},
}

@inproceedings{Mesaros:2010,
  title={Acoustic event detection in real life recordings},
  author={Mesaros, Annamaria and Heittola, Toni and Eronen, Antti and Virtanen, Tuomas},
  booktitle={18th European Signal Processing Conf.},
  pages={1267--1271},
  year={2010},
  doi={},
  abstract={This paper presents a system for acoustic event detection in recordings from real life environments. The events are modeled using a network of hidden Markov models; their size and topology is chosen based on a study of isolated events recognition. We also studied the effect of ambient background noise on event classification performance. On real life recordings, we tested recognition of isolated sound events and event detection. For event detection, the system performs recognition and temporal positioning of a sequence of events. An accuracy of 24% was obtained in classifying isolated sound events into 61 classes. This corresponds to the accuracy of classifying between 61 events when mixed with ambient background noise at 0dB signal-to-noise ratio. In event detection, the system is capable of recognizing almost one third of the events, and the temporal positioning of the events is not correct for 84% of the time.},
}

@inproceedings{Mesaros:2011,
  title={Latent semantic analysis in sound event detection},
  author={Mesaros, Annamaria and Heittola, Toni and Klapuri, Anssi},
  booktitle={19th European Signal Processing Conf.},
  pages={1307--1311},
  year={2011},
  month = aug
}

@inproceedings{Mesaros:2015,
  title={SOUND EVENT DETECTION IN REAL LIFE RECORDINGS USING COUPLED MATRIX FACTORIZATION OF SPECTRAL REPRESENTATIONS AND CLASS ACTIVITY ANNOTATIONS},
  author={Mesaros, Annamaria and Heittola, Toni and Dikmen, Onur and Virtanen, Tuomas},
  booktitle={IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2015},
  pages={151--155}, 
  abstract={Methods for detection of overlapping sound events in audio involve matrix factorization approaches, often assigning separated components to event classes. We present a method that bypasses the supervised construction of class models. The method learns the components as a non-negative dictionary in a coupled matrix factorization problem, where the spectral representation and the class activity annotation of the audio signal share the activation matrix. In testing, the dictionaries are used to estimate directly the class activations. For dealing with large amount of training data, two methods are proposed for reducing the size of the dictionary. The methods were tested on a database of real life recordings, and outperformed previous approaches by over 10%.},
}

@Misc{MIREX,
  title =	 {Music {I}nformation {R}etrieval {E}valuation e{X}change ({MIREX})},
  howpublished = {\url{http://music-ir.org/mirexwiki/}}
}

@book{Muller:2007,
	title = {Speaker Classification I: Fundamentals, Features, and Methods},
    year = {2007},
    doi = {10.1007/978-3-540-74200-5},
    publisher = {Springer Berlin Heidelberg},
    editor = {Christian M\"{u}ller}
}

@inproceedings{Mysore:2012,
  title={Variational Inference in Non-negative Factorial Hidden {Markov} Models for Efficient Audio Source Separation},
  author={Gautham J. Mysore and Maneesh Sahani},
  booktitle={Int. Conf. Machine Learning (ICML)},
  year={2012},
  pages={1887--1894},
  month={July},
}

@inproceedings{Murphy:2002,
  title={Linear-time inference in hierarchical {HMM}s},
  author={Murphy, Kevin P and Paskin, Mark A},
  booktitle={Advances in Neural Information Processing Systems},
  volume={2},
  pages={833--840},
  year={2002},
  doi={},
  abstract={The hierarchical hidden Markov model (HHMM) is a generalization of the hidden Markov model (HMM) that models sequences with structure at many length/time scales [FST98]. Unfortunately, the original inference algorithm is rather complicated, and takes O(T^3) time, where T is the length of the sequence, making it impractical for many domains. In this paper, we show how HHMMs are a special kind of dynamic Bayesian network (DBN), and thereby derive a much simpler inference algorithm, which only takes O(T) time. Furthermore, by drawing the connection between HHMMs and DBNs, we enable the application of many standard approximation techniques to further speed up inference.},
}

@book{Murphy:2012,
  title={Machine Learning: A Probabilistic Perspective},
  author={Murphy, K.P.},
  address = {Cambridge (Mass.), USA},
  year={2012},
  publisher={MIT Press}
}

@inproceedings{Okuno:2007,
  title={Computational auditory scene analysis and its application to robot audition: Five years experience},
  author={Okuno, Hiroshi G and Ogata, Tetsuya and Komatani, Kazunori},
  booktitle={Int. Conf. on Informatics Research for Development of Knowledge Soc. Infrastructure (ICKS 2007)},
  pages={69--76},
  year={2007},
  organization={IEEE},
  doi={10.1109/ICKS.2007.7},
  abstract={We have been engaged in research on computational auditory scene analysis to attain sophisticated robot/computer human interaction by manipulating real-world sound signals. The objective of our research is the understanding of an arbitrary sound mixture including non-speech sounds and music as well as voiced speech, obtained by robot's ears, that is, microphones embedded in the robot. We have coped with three main issues in computational auditory scene analysis, that is, sound source localization, separation, and recognition of separated sounds for a mixture of speech signals as well as polyphonic music signals. This paper overviews our results in robot audition, in particular, missing feature theory based integration of sound source separation and automatic speech recognition, and those in music information processing, in particular, drum sound equalizer.},
},
}

@article{Ostendorf:1996,
  title={From {HMM}'s to segment models: A unified view of stochastic modeling for speech recognition},
  author={Ostendorf, Mari and Digalakis, Vassilios V and Kimball, Owen A},
  journal={IEEE Transactions on Speech and Audio Processing},
  volume={4},
  number={5},
  pages={360--378},
  year={1996},
  publisher={IEEE},
  abstract={Many alternative models have been proposed to address some of the shortcomings of the hidden Markov model (HMM), which is currently the most popular approach to speech recognition. In particular, a variety of models that could be broadly classified as segment models have been described for representing a variable-length sequence of observation vectors in speech recognition applications. Since there are many aspects in common between these approaches, including the general recognition and training problems, it is useful to consider them in a unified framework. The paper describes a general stochastic model that encompasses most of the models proposed in the literature, pointing out similarities of the models in terms of correlation and parameter tying assumptions, and drawing analogies between segment models and HMMs. In addition, we summarize experimental results assessing different modeling assumptions and point out remaining open questions},
}

@INPROCEEDINGS{Parascandolo:2016,
author={G. Parascandolo and H. Huttunen and T. Virtanen},
booktitle={2016 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)},
title={Recurrent neural networks for polyphonic sound event detection in real life recordings},
year={2016},
pages={6440-6444},
month={March},}

@article{Phan:2014,
  title={Random Regression Forests for Acoustic Event Detection and Classification},
  author={Phan, Huy and Maasz, M and Mazur, Radoslaw and Mertins, Alfred},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2015},
  volume={23},
  number={1},
  pages={20--31},
  publisher={IEEE},
  keywords={birdonoff},
  abstract={Despite the success of the automatic speech recognition framework in its own application field, its adaptation to the problem of acoustic event detection has resulted in limited success. In this paper, instead of treating the problem similar to the segmentation and classification tasks in speech recognition, we pose it as a regression task and propose an approach based on random forest regression. Furthermore, event localization in time can be efficiently handled as a joint problem. We first decompose the training audio signals into multiple interleaved superframes which are annotated with the corresponding event class labels and their displacements to the temporal onsets and offsets of the events. For a specific event category, a random-forest regression model is learned using the displacement information. Given an unseen superframe, the learned regressor will output the continuous estimates of the onset and offset locations of the events. To deal with multiple event categories, prior to the category-specific regression phase, a superframe-wise recognition phase is performed to reject the background superframes and to classify the event superframes into different event categories. While jointly posing event detection and localization as a regression problem is novel, the superior performance on two databases ITC-Irst and UPC-TALP demonstrates the efficiency and potential of the proposed approach.},
}

@ARTICLE{Phan:2016,
   author = {H. Phan and L. Hertel and M. Maass and P. Koch and A. Mertins},
    title = "{Label Tree Embeddings for Acoustic Scene Classification}",
  journal = {ArXiv e-prints abs/1606.07908},
archivePrefix = "arXiv",
journal   = {CoRR},
  year      = {2016},
}

@INPROCEEDINGS{Piczak:2015,
author={K. J. Piczak},
booktitle={Int. Workshop on Machine Learning for Signal Processing (MLSP)},
title={Environmental sound classification with convolutional neural networks},
year={2015},
keywords={audio signal processing;cepstral analysis;neural nets;signal classification;audio clip;audio data;baseline implementation;convolutional layer;convolutional neural network;environmental recording;environmental sound classification;low level representation;max-pooling;mel-frequency cepstral coefficient;public dataset;segmented spectrogram;urban recording;Accuracy;Convolution;Convolutional codes;Neural networks;Pattern recognition;Training;Yttrium;classification;convolutional neural networks;environmental sound},
doi={10.1109/MLSP.2015.7324337},
ISSN={1551-2541},
month={Sept},}

@INPROCEEDINGS{Plinge:2014,
author={A. Plinge and R. Grzeszick and G. A. Fink},
booktitle={2014 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Bag-of-Features approach to acoustic event detection},
year={2014},
pages={3704--3708},
ISSN={1520-6149},
month={May},}

@Article{Poliner:2007,
  author =	 {G. Poliner and D. Ellis},
  title =	 {A discriminative model for polyphonic piano transcription},
  journal =	 {EURASIP J. Advances in Signal Processing},
  year =	 "2007",
  doi = 	 {10.1155/2007/48317},
  number =	 "8",
  month =	 jan,
  pages =	 "154--162"
}

@book{Rabiner:1993,
 author = {Rabiner, Lawrence and Juang, Biing-Hwang},
 title = {Fundamentals of Speech Recognition},
 year = {1993},
 isbn = {0-13-015157-2},
 publisher = {Prentice-Hall, Inc.},
 address = {Upper Saddle River, NJ, USA},
} 

@ARTICLE{Raczynski13,
  author={Raczynski, S.A. and Vincent, E. and Sagayama, S.},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  title={Dynamic {Bayesian} Networks for Symbolic Polyphonic Pitch Modeling},
  year={2013},
  volume={21},
  number={9},
  pages={1830--1840}
}

@ARTICLE{Rakotomamonjy:2015,
author={A. Rakotomamonjy and G. Gasso},
journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
title={Histogram of Gradients of Time-Frequency Representations for Audio Scene Classification},
year={2015},
volume={23},
number={1},
pages={142--153},
keywords={audio signal processing;gradient methods;support vector machines;time-frequency analysis;SVM;acoustic scenes;audio scene classification;audio signal;histogram of gradients;multiclass linear support vector machines;time-frequency representations;Feature extraction;Histograms;IEEE transactions;Mel frequency cepstral coefficient;Speech;Speech processing;Time-frequency analysis;Constant Q transform;histogram of gradient;support vector machines},
ISSN={2329-9290},
month={Jan},}

@INPROCEEDINGS{Roma:2013,
author={G. Roma and W. Nogueira and P. Herrera},
booktitle={2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
title={Recurrence quantification analysis features for environmental sound recognition},
year={2013},
keywords={audio signal processing;time series;RQA;auditory scene recognition;environmental audio recognition;environmental sound recognition;feature aggregation;nonlinear time series analysis technique;recurrence quantification analysis;unlabeled audio;Accuracy;Conf.s;Databases;Feature extraction;Mel frequency cepstral coefficient;Time series analysis},
doi={10.1109/WASPAA.2013.6701890},
ISSN={1931-1168},
month={Oct},}

@ARTICLE{Salamon:2016,
   author = {Salamon, J. and Bello, J.~P.},
    title = "{Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification}",
  journal = {ArXiv e-prints abs/1608.04363},
archivePrefix = "arXiv",
 primaryClass = "cs.SD",
 keywords = {Computer Science - Sound, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
     year = 2016,
    month = aug,
}

@INPROCEEDINGS{Schroder:2013,
author={J. Schr\"{o}der and N. Moritz and M. R. Sch\"{a}dler and B. Cauchi and K. Adiloglu and J. Anem\"{u}ller and S. Doclo and B. Kollmeier and S. Goetze},
booktitle={2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
title={On the use of spectro-temporal features for the IEEE AASP challenge `Detection and classification of acoustic scenes and events'},
year={2013},
pages={1-4},
ISSN={1931-1168},
month={Oct},}

@inproceedings{Sigtia:2014b,
  title={An {RNN}-based music language model for improving automatic music transcription},
  author={Sigtia, Siddharth and Benetos, Emmanouil and Cherla, Srikanth and Weyde, Tillman and Garcez, A and Dixon, Simon},
  booktitle={Int. Soc. for Music Information Retrieval Conf.},
  pages={53--58},
  year={2014},
  abstract={In this paper, we investigate the use of Music Language Models (MLMs) for improving Automatic Music Transcription performance. The MLMs are trained on sequences of symbolic polyphonic music from the Nottingham dataset. We train Recurrent Neural Network (RNN)-based models, as they are capable of capturing complex temporal structure present in symbolic music data. Similar to the function of language models in automatic speech recognition, we use the MLMs to generate a prior probability for the occurrence of a sequence. The acoustic AMT model is based on probabilistic latent component analysis, and prior information from the MLM is incorporated into the transcription framework using Dirichlet priors. We test our hybrid models on a dataset of multiple-instrument polyphonic music and report a significant 3% improvement in terms of F-measure, when compared to using an acoustic-only model.},
}

@article{Sigtia:2016b,
	author={S. Sigtia and E. Benetos and S. Dixon},
	journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
	title={An End-to-End Neural Network for Polyphonic Piano Music Transcription},
	volume={24},
	number={5},
	month={May},
	year={2016},
	pages = {927--939}
    }

@ARTICLE{Sigtia:2016,
   author = {Sigtia, S. and Stark, A.~M. and Krstulovic, S. and Plumbley, M.~D.
        },
    title = {Automatic Environmental Sound Recognition: Performance versus Computational Cost},
  journal = {IEEE/ACM Transactions on Audio, Speech and Language Processing},
archivePrefix = "arXiv",
   eprint = {1607.04589},
 primaryClass = "cs.SD",
 keywords = {Computer Science - Sound, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
     year = 2016,
    month = jul,
  abstract={In the context of the Internet of Things (IoT), sound sensing applications are required to run on embedded platforms where notions of product pricing and form factor impose hard constraints on the available computing power. Whereas Automatic Environmental Sound Recognition (AESR) algorithms are most often developed with limited consideration for computational cost, this article seeks which AESR algorithm can make the most of a limited amount of computing power by comparing the sound classification performance em as a function of its computational cost. Results suggest that Deep Neural Networks yield the best ratio of sound classification accuracy across a range of computational costs, while Gaussian Mixture Models offer a reasonable accuracy at a consistently small cost, and Support Vector Machines stand between both in terms of compromise between accuracy and computational cost. },
}

@article{Stowell:2013,
        title={Segregating Event Streams and Noise with a {Markov} Renewal Process Model},
        author={Stowell, D. and Plumbley, M. D.}, 
	journal = {J. Machine Learning Research},
        volume = {14},
        pages = {2213--2238},
        year={2013},
}

@article{Stowell:2015,
        title={Detection and classification of acoustic scenes and events},
        journal={{IEEE} Transactions on Multimedia},
        author={Stowell, D. and Giannoulis, D. and Benetos, E. and Lagrange, M. and Plumbley, M. D.},
        year={2015},
        volume={17},
        number={10},
        month={October},
        pages={1733--1746},
        keywords={dcase,badchall},
        abstract={For intelligent systems to make best use of the audio modality, it is important that they can recognise not just
speech and music, which have been researched as specific tasks, but also general sounds in everyday environments.
To stimulate research in this field we conducted a public research challenge: the IEEE Audio and Acoustic Signal
Processing Technical Committee challenge on Detection and Classification of Acoustic Scenes and Events (DCASE).
In this paper we report on the state of the art in automatically classifying audio scenes, and automatically detecting
and classifying audio events. We survey prior work as well as the state of the art represented by the submissions to
the challenge from various research groups. We also provide detail on the organisation of the challenge, so that our
experience as challenge hosts may be useful to those organising challenges in similar domains. We created new audio
datasets and baseline systems for the challenge: these, as well as some submitted systems, are publicly available
under open licenses, to serve as benchmark for further research in general-purpose machine listening.},
}

@inproceedings{Stowell:2015b,
        author={Stowell, D. and Clayton, D.},
        title={Acoustic event detection for multiple overlapping similar sources},
        booktitle={IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
        year={2015},
  keywords={badchall},
        abstract={Many current paradigms for acoustic event detection (AED) are not adapted to the organic variability of natural sounds, and/or they assume a limit on the number of simultaneous sources: often only one source, or one source of each type, may be active. These aspects are highly undesirable for applications such as bird population monitoring. We introduce a simple method modelling the onsets, durations and offsets of acoustic events to avoid intrinsic limits on polyphony or on inter-event temporal patterns. We evaluate the method in a case study with over 3000 zebra finch calls. In comparison against a HMM-based method we find it more accurate at recovering acoustic events, and more robust for estimating calling rates. },
}

@article{Stowell:2016,
        title={Detailed temporal structure of communication networks in groups of songbirds},
        author={Stowell, D. and Gill, L. F. and Clayton, D.},
        journal={J. Royal Soc. Interface},
        volume={13},
        number={119},
        year={2016},
        doi={10.1098/rsif.2016.0296},
        abstract={Animals in groups often exchange calls, in patterns whose temporal structure may be influenced by contextual factors such as physical location and the social network structure of the group. We introduce a model-based analysis for temporal patterns of animal call timing, originally developed for networks of firing neurons. This has advantages over cross-correlation analysis in that it can correctly handle common-cause confounds and provides a generative model of call patterns with explicit parameters for the influences between individuals. It also has advantages over standard Markovian analysis in that it incorporates detailed temporal interactions which affect timing as well as sequencing of calls. Further, a fitted model can be used to generate novel synthetic call sequences. We apply the method to calls recorded from groups of domesticated zebra finch (Taeniopygia guttata) individuals. We find that the communication network in these groups has stable structure that persists from one day to the next, and that “kernels” reflecting the temporal range of influence have a characteristic structure for a calling individual’s effect on itself, its partner, and on others in the group. We further find characteristic patterns of influences by call type as well as by individual.},
}

@article{Stowell:2016b,
        title={On-bird Sound Recordings: Automatic Acoustic Recognition of Activities and Contexts},
        author={Stowell, D. and Benetos, E. and Gill, L. F.}, 
	journal = {IEEE/ACM Transactions on Audio, Speech and Language Processing},
        year={accepted},
}

@inproceedings{Sturm:2014,
author="Sturm, Bob L.",
title="A Survey of Evaluation in Music Genre Recognition",
bookTitle="10th Int. Workshop on Adaptive Multimedia Retrieval: Semantics, Context, and Adaptation (AMR 2012), Revised Selected Papers",
year="2014",
publisher="Springer Int. Publishing",
pages="29--66",
isbn="978-3-319-12093-5",
doi="10.1007/978-3-319-12093-5_2",
}



@article{Tranter:2006,
  title={An overview of automatic speaker diarization systems},
  author={Tranter, Sue E and Reynolds, Douglas A},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={14},
  number={5},
  pages={1557--1565},
  year={2006},
  publisher={IEEE},
  abstract={Audio diarization is the process of annotating an input audio channel with information that attributes (possibly overlapping) temporal regions of signal energy to their specific sources. These sources can include particular speakers, music, background noise sources, and other signal source/channel characteristics. Diarization can be used for helping speech recognition, facilitating the searching and indexing of audio archives, and increasing the richness of automatic transcriptions, making them more readable. In this paper, we provide an overview of the approaches currently used in a key area of audio diarization, namely speaker diarization, and discuss their relative merits and limitations. Performances using the different techniques are compared within the framework of the speaker diarization task in the DARPA EARS Rich Transcription evaluations. We also look at how the techniques are being introduced into real broadcast news systems and their portability to other domains and tasks such as meetings and speaker verification},
}

@article{Vincent:2012,
  title={The signal separation evaluation campaign (2007--2010): Achievements and remaining challenges},
  author={Vincent, Emmanuel and Araki, Shoko and Theis, Fabian and Nolte, Guido and Bofill, Pau and Sawada, Hiroshi and Ozerov, Alexey and Gowreesunker, Vikrham and Lutter, Dominik and Duong, Ngoc QK},
  journal={Signal Processing},
  volume={92},
  number={8},
  pages={1928--1936},
  year={2012},
  doi={10.1016/j.sigpro.2011.10.007},
  abstract={We present the outcomes of three recent evaluation campaigns in the field of audio and biomedical source separation. These campaigns have witnessed a boom in the range of applications of source separation systems in the last few years, as shown by the increasing number of datasets from 1 to 9 and the increasing number of submissions from 15 to 34. We first discuss their impact on the definition of a reference evaluation methodology, together with shared datasets and software. We then present the key results obtained over almost all datasets. We conclude by proposing directions for future research and evaluation, based in particular on the ideas raised during the related panel discussion at the Ninth Int. Conf. on Latent Variable Analysis and Signal Separation (LVA/ICA 2010).},
}

@inproceedings{Ye:2015,
 author = {Ye, Jiaxing and Kobayashi, Takumi and Murakawa, Masahiro and Higuchi, Tetsuya},
 title = {Acoustic Scene Classification Based on Sound Textures and Events},
 booktitle = {ACM Int. Conf. Multimedia},
 year = {2015},
 location = {Brisbane, Australia},
 pages = {1291--1294},
 doi = {10.1145/2733373.2806389},
 address = {New York, NY, USA},
 keywords = {acoustic scene classification, constant-q transform, histogram of gradients, model aggregation}
} 


@article{Yu:2010,
        doi = {10.1016/j.artint.2009.11.011},
        year = 2010,
        month = {feb},
        publisher = {Elsevier {BV}},
        volume = {174},
        number = {2},
        pages = {215--243},
        author = {Shun-Zheng Yu},
        title = {Hidden semi-{M}arkov models},
        journal = {Artificial Intelligence},
  abstract={As an extension to the popular hidden Markov model (HMM), a hidden semi-Markov model (HSMM) allows the underlying stochastic process to be a semi-Markov chain. Each state has variable duration and a number of observations being produced while in the state. This makes it suitable for use in a wider range of applications. Its forward–backward algorithms can be used to estimate/update the model parameters, determine the predicted, filtered and smoothed probabilities, evaluate goodness of an observation sequence fitting to the model, and find the best state sequence of the underlying stochastic process. Since the HSMM was initially introduced in 1980 for machine recognition of speech, it has been applied in thirty scientific and engineering areas, such as speech recognition/synthesis, human activity recognition/prediction, handwriting recognition, functional MRI brain mapping, and network anomaly detection. There are about three hundred papers published in the literature. An overview of HSMMs is presented in this paper, including modelling, inference, estimation, implementation and applications. It first provides a unified description of various HSMMs and discusses the general issues behind them. The boundary conditions of HSMM are extended. Then the conventional models, including the explicit duration, variable transition, and residential time of HSMM, are discussed. Various duration distributions and observation models are presented. Finally, the paper draws an outline of the applications.},
}

@book{Yu:2015,
  title={Automatic Speech Recognition: A Deep Learning Approach},
  author={Dong Yu and Li Deng},
  year={2015},
  publisher={Springer-Verlag},
  doi={10.1007/978-1-4471-5779-3},
  issn={1860-4862},
  address={London, UK},
}

@INPROCEEDINGS{Zhang:2015,
author={H. Zhang and I. McLoughlin and Y. Song},
booktitle={2015 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)},
title={Robust sound event recognition using convolutional neural networks},
year={2015},
pages={559--563},
ISSN={1520-6149},
month={April},}

